import logging

from typing import Optional
from concurrent.futures import ThreadPoolExecutor
from queue import Queue, Empty

from crawler.sources import (
    Source
)

log = logging.getLogger(__name__)

class Crawler():
    def __init__(
        self,
        sources: list[Source],
        num_workers: int = 4,
        queue_timeout: int = 30 
    ) -> None:
        log.debug(
            f"Crawler was successfully created with {num_workers}"
        )
        self._sources = sources # Should be got from RequestManager
        self._queue_timeout = queue_timeout
        self._pool = ThreadPoolExecutor(
            max_workers=num_workers
        )
        self._crawl_contents = list()
        self._crawled_sources = set()

    @property
    def result(
        self
    ) -> list:
        return self._crawl_contents

    @property
    def is_crawled(
        self
    ) -> bool:
        return True if len(self._crawl_contents) else False

    def _worker_crawl(
        self,
        source: Source
    ) -> Optional[dict]:
        content = source.download()
        if not content:
            log.info(
                f"Content was not downloaded for {source.title}"
            )
            return
        
        parsed = source.parse(data=content)
        if not parsed:
            log.info(
                f"Content was not parsed for {source.title}"
            )
            return
        
        return parsed

    def _aggregate_content(
        self,
        parsed: Optional[dict]
    ) -> None:
        if not parsed:
            self._crawl_contents.append(
                parsed
            )
        
    def crawl(
        self,
    ) -> None:
        self._crawl_contents.clear()
        self._crawled_sources.clear()
        
        source_queue = Queue()
        for source in self._sources:
            source_queue.put(source)
        
        while True:
            try:
                source = source_queue.get(
                    timeout=self._queue_timeout
                )
                self._crawled_sources.add(
                    source.title
                )
                
                job = self._pool.submit(
                    self._worker_crawl, 
                    source=source,
                )
                job.add_done_callback(
                    self._aggregate_content
                )
            except Empty:
                return
            except Exception as exception:
                log.error(f"Crawl was interrupted with exception. {exception}")
                continue