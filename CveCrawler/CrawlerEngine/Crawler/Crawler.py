import logging

from concurrent.futures import ThreadPoolExecutor
from queue import Queue, Empty

from Downloaders import IDownloader
from Parsers import IParser
from Source import Source

log = logging.getLogger(__name__)

class Crawler():
    def __init__(
        self,
        source_queue: Queue,
        num_workers: int = 4
    ) -> None:
        self._queue = source_queue # Should be got from RequestManager with sources
        self._pool = ThreadPoolExecutor(
            max_workers=num_workers
        )
        #self._crawled_sources = set()

    def _worker_crawl(
        self,
        source: Source,
    ) -> None:
        content = source._downloader.download()
        if content is None:
            return
        
        parsed = source._parser.parse(
            data=content,
        )
        if parsed is None:
            return

    def crawl(
        self,
    ) -> None:
        while True:
            try:
                source = self._queue.get(timeout=10)
                #if source not in self._crawled_sources:
                print(f"Scraping URL: {source._seed_url} ({source.title})")
                #self._crawled_sources.add(source.title)
                job = self._pool.submit(
                    self._worker_crawl, 
                    source=source,
                )
                #job.add_done_callback(self.callback) ???
            except Empty:
                return
            except Exception as e:
                print(e)
                continue